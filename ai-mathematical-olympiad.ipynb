{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"sourceType":"competition"},{"sourceId":263434,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":225305,"modelId":247048}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T16:45:45.255377Z","iopub.execute_input":"2025-11-22T16:45:45.256108Z","iopub.status.idle":"2025-11-22T16:45:45.262233Z","shell.execute_reply.started":"2025-11-22T16:45:45.256078Z","shell.execute_reply":"2025-11-22T16:45:45.261598Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport sys\nimport re\nimport time\nimport polars as pl\nimport pandas as pd\nimport torch\nimport math\nimport numpy as np\nimport sympy\nimport itertools\nimport collections\nimport fractions\nimport gc\nimport signal\nfrom collections import Counter\nfrom io import StringIO\nimport kaggle_evaluation.aimo_3_inference_server\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\nMODEL_PATH = \"/kaggle/input/math_numina_7b_tir/pytorch/default/1/NuminaMath-7B-TIR\"\n\nclass ProductionAIMO3Solver:\n    def __init__(self):\n        self.model = None\n        self.tokenizer = None\n        self.is_loaded = False\n        self.problem_cache = {}\n\n    def load(self):\n        if self.is_loaded: return\n        \n        print(f\"‚è≥ Loading NuminaMath-7B-TIR...\")\n        \n        if not os.path.exists(MODEL_PATH):\n            print(f\"‚ùå Model not found at {MODEL_PATH}\")\n            return\n\n        try:\n            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n            \n            max_mem = {0: \"11GiB\", 1: \"11GiB\", \"cpu\": \"30GiB\"}\n            self.model = AutoModelForCausalLM.from_pretrained(\n                MODEL_PATH,\n                torch_dtype=torch.float16,\n                device_map=\"auto\",\n                max_memory=max_mem,\n                local_files_only=True,\n                low_cpu_mem_usage=True\n            )\n            print(\"‚úÖ Model Loaded (Float16 Split)!\")\n            self.is_loaded = True\n            \n        except Exception as e:\n            print(f\"‚ùå Load Failed: {e}\")\n            return\n\n    def run_python_code_with_timeout(self, code, timeout=10):\n        \"\"\"REAL Python execution with TIMEOUT\"\"\"\n        sys.set_int_max_str_digits(0)\n        local_scope = {\n            \"math\": math, \"np\": np, \"sympy\": sympy, \"itertools\": itertools,\n            \"collections\": collections, \"fractions\": fractions, \"print\": print,\n            \"sys\": sys, \"Counter\": Counter\n        }\n        \n        old_stdout = sys.stdout\n        redirected_output = sys.stdout = StringIO()\n        \n        def timeout_handler(signum, frame):\n            raise TimeoutError(\"Code execution timed out\")\n        \n        signal.signal(signal.SIGALRM, timeout_handler)\n        signal.alarm(timeout)\n        \n        try:\n            code = re.sub(r\"```python|```\", \"\", code)\n            exec(code, local_scope)\n            signal.alarm(0)\n            sys.stdout = old_stdout\n            return redirected_output.getvalue().strip(), \"SUCCESS\"\n        except TimeoutError:\n            signal.alarm(0)\n            sys.stdout = old_stdout\n            return \"Execution timed out\", \"TIMEOUT\"\n        except Exception as e:\n            signal.alarm(0)\n            sys.stdout = old_stdout\n            return f\"Runtime Error: {e}\", \"ERROR\"\n\n    def extract_answer(self, text):\n        try:\n            numbers = re.findall(r'\\b\\d{1,5}\\b', text)\n            if numbers:\n                answer = int(numbers[-1])\n                if 0 <= answer <= 99999:\n                    return answer\n        except:\n            pass\n        return None\n\n    def solve(self, problem_text, max_retries=1):\n        if not self.is_loaded: \n            self.load()\n        if not self.is_loaded: \n            return 0\n        \n        problem_hash = hash(problem_text)\n        if problem_hash in self.problem_cache:\n            return self.problem_cache[problem_hash]\n        \n        print(f\"üß† Solving: {problem_text[:50]}...\")\n        start_time = time.time()\n        \n        prompt = f\"\"\"Solve this problem with concise Python code. Print only the final integer answer.\n\nProblem: {problem_text}\n\nPython code:\n```python\n\"\"\"\n        \n        for attempt in range(max_retries + 1):\n            gc.collect()\n            torch.cuda.empty_cache()\n            \n            try:\n                inputs = self.tokenizer(\n                    prompt, \n                    return_tensors=\"pt\", \n                    max_length=1024,\n                    truncation=True,\n                    padding=True\n                ).to(self.model.device)\n                \n                with torch.no_grad():\n                    generated_ids = self.model.generate(\n                        **inputs,\n                        max_new_tokens=512,\n                        do_sample=False,\n                        pad_token_id=self.tokenizer.eos_token_id,\n                        repetition_penalty=1.1\n                    )\n                \n                response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n                response = response[len(prompt):] if response.startswith(prompt) else response\n                \n                code_match = re.search(r\"```python(.*?)```\", response, re.DOTALL)\n                if code_match:\n                    code = code_match.group(1).strip()\n                    if code:\n                        output, status = self.run_python_code_with_timeout(code, timeout=15)\n                        \n                        if status == \"SUCCESS\":\n                            answer = self.extract_answer(output)\n                            if answer is not None:\n                                elapsed = time.time() - start_time\n                                print(f\"    ‚úÖ Code success: {answer} ({elapsed:.1f}s)\")\n                                self.problem_cache[problem_hash] = answer\n                                return answer\n                        else:\n                            print(f\"    ‚ùå {status}: {output}\")\n                \n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    print(\"    ‚ö†Ô∏è OOM - clearing cache\")\n                    torch.cuda.empty_cache()\n                    continue\n                else:\n                    print(f\"    ‚ùå Runtime error: {e}\")\n            except Exception as e:\n                print(f\"    ‚ùå Generation error: {e}\")\n                continue\n        \n        elapsed = time.time() - start_time\n        print(f\"‚ùå Failed after {elapsed:.1f}s\")\n        return 0\n\n# ==========================================\n# ACTUAL REFERENCE.CSV TESTING\n# ==========================================\nsolver = ProductionAIMO3Solver()\n\ndef test_reference_problems():\n    \"\"\"ACTUALLY test on reference.csv problems\"\"\"\n    try:\n        # Load the reference problems\n        reference_df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv')\n        print(f\"üìä TESTING ON {len(reference_df)} REFERENCE PROBLEMS\")\n        print(\"=\" * 80)\n        \n        correct = 0\n        total = len(reference_df)\n        total_time = 0\n        \n        for i in range(min(3, total)):  # Test first 3 to save time\n            row = reference_df.iloc[i]\n            problem_text = row['problem']\n            true_answer = row['answer']\n            \n            print(f\"\\nüî¢ REFERENCE PROBLEM {i+1}/{min(3, total)}\")\n            print(f\"   TRUE ANSWER: {true_answer}\")\n            print(f\"   PROBLEM: {problem_text[:80]}...\")\n            \n            start_time = time.time()\n            predicted = solver.solve(problem_text)\n            elapsed = time.time() - start_time\n            total_time += elapsed\n            \n            status = \"‚úÖ CORRECT\" if predicted == true_answer else \"‚ùå WRONG\"\n            print(f\"   {status} | PREDICTED: {predicted} | TIME: {elapsed:.1f}s\")\n            \n            if predicted == true_answer:\n                correct += 1\n            \n            torch.cuda.empty_cache()\n            time.sleep(1)\n        \n        accuracy = correct / min(3, total) * 100\n        avg_time = total_time / min(3, total)\n        \n        print(f\"\\nüéØ REFERENCE SET PERFORMANCE: {correct}/{min(3, total)} correct ({accuracy:.1f}%)\")\n        print(f\"‚è±Ô∏è  Average time: {avg_time:.1f}s per problem\")\n        \n        if accuracy >= 50:\n            print(\"üéâ READY FOR COMPETITION SUBMISSION!\")\n        else:\n            print(\"‚ö†Ô∏è  Needs improvement before submission\")\n            \n        return accuracy\n        \n    except Exception as e:\n        print(f\"‚ùå Error loading reference problems: {e}\")\n        return 0\n\ndef predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame:\n    id_val = id_.item(0)\n    problem_text = problem.item(0)\n    try:\n        answer = solver.solve(problem_text)\n    except:\n        answer = 0\n    return pl.DataFrame({'id': [id_val], 'answer': [answer]})\n\ninference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    print(\"üöÄ Starting Production Server...\")\n    inference_server.serve()\nelse:\n    print(\"üî¨ Local Test Mode...\")\n    # FINALLY TEST ON ACTUAL REFERENCE PROBLEMS\n    accuracy = test_reference_problems()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T16:48:38.458792Z","iopub.execute_input":"2025-11-22T16:48:38.459739Z","iopub.status.idle":"2025-11-22T16:52:33.767217Z","shell.execute_reply.started":"2025-11-22T16:48:38.459702Z","shell.execute_reply":"2025-11-22T16:52:33.766546Z"}},"outputs":[{"name":"stdout","text":"üî¨ Local Test Mode...\nüìä TESTING ON 10 REFERENCE PROBLEMS\n================================================================================\n\nüî¢ REFERENCE PROBLEM 1/3\n   TRUE ANSWER: 336\n   PROBLEM: Let $ABC$ be an acute-angled triangle with integer side lengths and $AB<AC$. Poi...\n‚è≥ Loading NuminaMath-7B-TIR...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ae1af5597dc4717b52c2c49f5e2ef97"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Model Loaded (Float16 Split)!\nüß† Solving: Let $ABC$ be an acute-angled triangle with integer...\n‚ùå Failed after 63.0s\n   ‚ùå WRONG | PREDICTED: 0 | TIME: 107.2s\n\nüî¢ REFERENCE PROBLEM 2/3\n   TRUE ANSWER: 32951\n   PROBLEM: Define a function $f \\colon \\mathbb{Z}_{\\geq 1} \\to \\mathbb{Z}_{\\geq 1}$ by\n\\beg...\nüß† Solving: Define a function $f \\colon \\mathbb{Z}_{\\geq 1} \\t...\n‚ùå Failed after 62.5s\n   ‚ùå WRONG | PREDICTED: 0 | TIME: 62.5s\n\nüî¢ REFERENCE PROBLEM 3/3\n   TRUE ANSWER: 21818\n   PROBLEM: A tournament is held with $2^{20}$ runners each of which has a different running...\nüß† Solving: A tournament is held with $2^{20}$ runners each of...\n‚ùå Failed after 62.5s\n   ‚ùå WRONG | PREDICTED: 0 | TIME: 62.5s\n\nüéØ REFERENCE SET PERFORMANCE: 0/3 correct (0.0%)\n‚è±Ô∏è  Average time: 77.4s per problem\n‚ö†Ô∏è  Needs improvement before submission\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}